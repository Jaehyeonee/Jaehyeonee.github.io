---
title: ML.경사 하강법에 대해 알아보자 [Gradient Descent, SGD, mini-batch]
author: jaehyeonee
date: 2024-03-11 15:00:00 +0800
categories: [Study, DL]
tags: [DL]
pin: true
math: true
mermaid: true
# image:ㄴ
#   path: 
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: Responsive rendering 
---

모델을 훈련시킨다는 것은 비용함수를 최적화하는 파라미터를 찾는 것을 의미한다. 

비용함수를 최소화하는 최적화 파라미터를 찾기 위한 방법으로는 크게 3가지가 있다.

1. 정규방정식
2. SVD 
3. 경사 하강법

그 중 가장 많이 사용되는 경사 하강법에 대해 정리해보자. 

경사하강법은 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 ‘최적화 알고리즘’이다.

경사하강법은 비용 함수를 최소화하기 위해서 반복해 파라미터를 조정해나간다.

파라미터 백터θ 에 대해 비용함수의 현재 gradient를 계산한다. 

gradient가 감소하는 방향으로 전개되며, gradient=0이 되는 지점이 비용함수가 최소가 되는 지점이다.

무작위 초기화로 이뤄지며 비용함수가 감소되는 방향으로 알고리즘이 최솟값에 수렴할때까지 점진적으로 향상시킨다. 

경사하강법에서의 중요한 하이퍼 파라미터 값인 `‘learning rate’` 학습률은 기울기에 비례하며, 

파라미터가 최솟값에 가까워질 수록 스텝의 크기가 점점 줄어든다.

- 학습률이 너무 작으면, 알고리즘에 수렴하기 위해 반복을 많이 진행하여 시간이 오래걸린다.

### 경사하강법의 문제점 

![image](https://github.com/Jaehyeonee/FeelMyBrain/assets/92504386/5cbc9768-6641-4149-8622-1d8d4ba58137)

![image](https://github.com/Jaehyeonee/FeelMyBrain/assets/92504386/6785de42-2968-484f-9e4b-b88cf52c679a)

파라미터가 무작위 초기화가 이뤄짐에 따라 

- 알고리즘이 왼쪽에서 시작이 된다면 
  → 전역 최솟값에 도달하지 못하고, 지역 최솟값에 도달하여 알고리즘의 최적화를 이뤄내지 못한다.

- 알고리즘이 오른쪽에서 시작이 된다면 
  → 기울기가 완만하기 때문에, learning rate가 매우 작아 시간이 오래걸리고, 일찍 멈추게 되어 최솟값에 도달하지 못한다. 


선형회귀를 위한 MSE 비용함수는 Convex funtion 볼록함수로, 곡선의 어떤 두 점을 선택해서 직선을 그엇을 때 곡선을 가로지르지 않는다는 특성이 있다. 

1. 무조건 하나의 전역 최솟값을 가진다
2. 연속된 함수이며 기울기가 값자기 변하지 않는다

따라서, 
경사하강법이 전역 최솟값에 가깝게 접근할 수 있다는 것을 보장한다. 
(학습률이 너무 크지 않고, 학습 시간이 충분하다면)

![image](https://github.com/Jaehyeonee/FeelMyBrain/assets/92504386/d49e5bf3-2ffc-43b0-af7d-43e574fd72e5)

특성들의 스케일이 다를수록 길쭉한 볼록 함수 형태이다.

오른쪽 그래프는 특성들의 스케일이 다른 함수의 경사하강법 진행을 보여준다.

처음에는 전역 최솟값의 방향에 직각으로 향하다가 평편한 골짜기를 길게 돌아서 나간다.

최솟값에 도달은 하지만 시간이 매우 오래걸릴 것이다.

따라서 경사하강법을 사용할 때는 모든 특성이 반드시 같은 스케일을 같도록 만들어야 한다.(ex.standadScaler)

모델 훈련은 훈련세트에 대해 비용함수를 최소화하는 모델 파라미터의 조합을 파라미터 공간에서 찾는 것이다.

모델이 가진 파라미터가 많을수록 공간의 차원이 커지고 검색이 더 어려워진다. 

    *선형회귀의 경우 비용함수가 convex function이기 때문에 최솟값을 찾기 쉬움



## 배치 경사 하강법

매 경사 하강법 스텝에서 전체 훈련 세트 X 에 대해 계산한다.

매 스텝에서 훈련데이터 전체를 사용한다는 특징을 가지며, 이는 전역 최솟값에 도달하게는 하지만 학습 시간이 오래걸리게 한다.

`편도함수`: 각 모델 파라미터(차원파라미터) θj 가 조금씩 변경될 때 비용함수가 얼마나 바뀌는지 계산.

![image](https://github.com/Jaehyeonee/FeelMyBrain/assets/92504386/3b29a7c4-7524-4595-b822-6cda9f580437)


그레디언트 벡터는 비용함수의 편도함수를 모두 담고 있다 

![image](https://github.com/Jaehyeonee/FeelMyBrain/assets/92504386/c7d074a3-9bc7-4e81-8025-b18cb5a7ffd2)
ㄴ비용함수의 그레디언트 벡터


#### 경사하강법 최적파라미터 theta찾기

```python
import numpy as np

lr = 0.1
n_iterationss= 100 
m = 100

theta = np.random.randn(2,1)

for interation in range(n_iterationss):
  gradient = 2/m * X.T.dot(X_b.dot(theta)-y) # 비용함수의 그레디언트 벡터
  theta = theta -lr * gradients
	print("iteration: ",iteration, theta)

theta
```

#### 단점

배치 경사하강법의 가장 큰 문제는 매 스텝에서 전체 훈련 세트를 이용해 그레디언트를 계산해서 매우 느려짐.





## 확률적 경사하강법 Stochastic Gradient Descent

배치 경사하강법의 문제점을 해결하기 위해 등장했다.

매 스텝에서 한 개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 그레디언트를 계산한다.

매 반복에서 하나의 샘플만 메모리에 있으면 되므로 매운 큰 훈련 세트도 훈련이 가능하다.

하지만, 

`“확률적”` 이기 때문에 불안정하다. 

비용함수가 최솟값에 도달할 때까지 부드럽게 감소하지 않고 위아래로 진동(<span style="color:red">jittering</span>)하며 평균적으로 감소하게 된다. 

최솟값에 가까워지기는 하나, 진동 때문에  최적치에 도달하지는 못한다.

비용함수가 불규칙할 때 =jittering 알고리즘은 오히려 지역최솟값을 건너뛸 수 있도록 해준다.

따라서 확률적 경사하강법이 배치 경사하강법에 비해 전역 최솟값을 찾을 가능성이 높다. 

`무작위성`은 지역 최솟값은 탈출시켜주지만, 알고리즘을 전역 최솟값에 다다르지 못하게 한다는 점에서 좋지 않다. 

  → 학습률을 점진적으로 감소시켜 해결. [<span style="color:red">학습 스케줄링</span>]

  → 시작할 때는 학습률을 크게 해 수렴을 빠르게 돕고 지역 최솟값에 빠지지 않게 함.

  → 점차 학습률을 작게하여 알고리즘이 전역 최솟값에 도달하게 한다.



## 미니배치 경사 하강법

`‘미니배치’` 라 부르는 임의의 작은 샘플 세트에 대해 그레디언트를 계산한다. 

GPU를 사용해서 얻는 성능 향상에 기여가 큰 알고리즘이다.

미니 배치를 어느 정도 크게 하면, 파라미터 공간에서 SGD보다는 덜 불규칙하게 움직여 SGD보다 최솟값에 더 가까이 도달하게 될 것이다.



## 정리
![image](https://github.com/Jaehyeonee/FeelMyBrain/assets/92504386/712eac46-cf4b-4446-804b-e8918dae1cc8)


> 3가지 경사 하강법 알고리즘 모두 최솟값 근처에 도달했지만, 배치 경사 하강법의 경로가 실제로 최솟값에서 멈춘 반면 SGD와 미니배치 경사하강법은 근처에서 맴돈다. 
> 
> 하지만, 배치경사하강법에는 매 스텝에서 많은 시간이 소요된다.
>  
> 따라서 SGD와 미니배치 경사하강법에 적절한 학습 스케줄링을 하면 최솟값에 도달할 수 있다.
>