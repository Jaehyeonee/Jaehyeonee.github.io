---
title: NLP. 사전 학습 모델(NLP, BERT 모델 파인튜닝)
author: Jaehyeonee
date: 2023-10-17 13:35:00 +0800
categories: [Study, DL, NLP]
tags: [Algorithm]
pin: true
math: true
mermaid: true
# image:
#   path: 
#   lqip: data:image/webp;base64,UklGRpoAAABXRUJQVlA4WAoAAAAQAAAADwAABwAAQUxQSDIAAAARL0AmbZurmr57yyIiqE8oiG0bejIYEQTgqiDA9vqnsUSI6H+oAERp2HZ65qP/VIAWAFZQOCBCAAAA8AEAnQEqEAAIAAVAfCWkAALp8sF8rgRgAP7o9FDvMCkMde9PK7euH5M1m6VWoDXf2FkP3BqV0ZYbO6NA/VFIAAAA
#   alt: Responsive rendering 
---

# 사전 학습 모델(pre-training)
사전 학습 모델이란 기존에 Xavier 등 임의의 값으로 초기화하던 모델의 가중치들을 다른 문제에 학습시킨 가중치들로 초기화하는 방법이다.
사전 학습한 모델을 __사전 학습 문제(pre-train task)__ 라고 하고,
사전 학습한 가중치를 활용해 학습하고자 하는 본 문제를 __하위 문제(downstream task)__ 라고 한다.

사전 학습은 최근 대부분의 자연어 처리 연구에서 활용되고 있다.
특히, 언어 모델을 사전 학습의 핵심 문제로 활용하고 있는데, 유명한 __`버트 BERT`__ 와 __`GPT`__ 를 포함해 대부분의 자연어 처리 모델이 언어 모델을 사전 학습한 모델을 활용한다.

*** 

## 사전 학습 모델을 언제 사용하는가?
학습 데이터만 사용해 학습을 진행해야 하는데, 기존 데이터에 비해 턱없이 데이터가 부족한 상황에 유용한다.

사전학습을 하지 않고 학습하는 과정
> 1. EDA과정을 통해 분석, 전처리 --> 2. 모델을 구현 해 모델 생성
임의로 초기화된 가중치들을 가지고 있는 모델을 유사도 데이터를 활용해 학습한다.

__사전학습__ 이란, 위 과정 중 최초로 생성된 모델의 가중치를 임의의 값으로 초기화하는 것이 아니라! 사전에 학습된 가중치를 활용하는 방식이다.
> 사전 학습한 가중치를 모델의 초깃값으로 활용하는 것
{: .prompt-tip }

주의해야 할 점은, 모델의 모든 가중치를 다 사용하는 것이 아니다. 최종 출력값의 경우 문제마다 형태가 모두 다르기 때문에, 대부분의 가중치를 활용할 수 있지만 모델의 __최종 출력값__ 을 뽑는 가중치 층을 제외하고 사용해야 한다.

사전 학습한 모델의 최종 출력 층을 제외한 층들의 가중치를 하위 문제 모델의 첫 가중치로 초기화한 뒤 학습하면, 사전 학습을 활용한 모델 학습이 된다.


## 언어 모델
대부분의 자연어 처리 모델이 언어 모델을 사전 학습한 모델을 활용한다.
언어 모델은 간단히, 특정 단어가 주어졌을 때 단어가 어떤 단어인지를 예측하는 것을 해결하는 문제이다.

예를 들어,
> "오늘 아침 반찬 간이 조금 싱거워" 라는 문장이 있을 때, "오늘 아침 반찬 간이" 라는 단어들을 통해 "싱거워" 라는 단어를 모델이 예측하며 학습하게 된다.

이러한 학습을 통해 모델은 언어에 대한 전반적인 이해 (NLU)를 하게 되고, 사전 학습된 지식을 기반으로 하위 문제에 대한 성능을 향상시킨다.

언어 모델의 경우 라벨이 필요 없는 대표적인 비지도 학습 문제 중 하나로, 데이터의 제약이 없고 __언어에 대한 전반적인 이해를 사전 학습하는 것__ 이기 때문에 하위 문제 모델의 성능도 대부분 향상시킨다.

  * 언어 모델을 예측하기 위한 데이터는 라벨이 필요없다(텍스트 데이터 자체가 라벨이 됨) >> "비지도 학습"
  

## 사전 학습한 가중치를 활용해 하위 문제를 학습하는 방법?
사전 학습한 가중치를 활용하는 방법은 크게 2가지 이다.
* 특징 기반(feature-based) 방법
* 미세 조정(fine-tuning) 방법

### 특징 기반(feature-based) 방법
특징 기반 방법이란, 사전 학습된 특징을 하위 문제의 부가적인 특징으로 활용하는 방법이다.
`특징` 이란 모델 중간에 나오는 특징값(feature value)라고 생각하면 된다.
ex) 단어에 대한 특징값 = 임베딩 벡터
특징 기반의 사전 학습 활용 방법의 대표적인 예는 `word2vec`
> 학습한 임베딩 특징을 우리가 학습하고자 하는 모델의 임베딩 특징으로 활용하는 방법이다.



### 미세 조정(fine-tuning) 방법
사전 학습한 모든 가중치와 더불어 하위 문제를 위한 최소한의 가중치를 추가해서 모델을 추가로 학습(fine-tuning) 하는 방법이다.
> ex)감정분석문제의 사전학습 가중치 + 텍스트 유사도(하위문제)를 위한 부가적인 가중치 => 텍스트 유사도 문제 학습하는 것


대부분의 자연어 처리 연구에서는 __트랜스포머__ 모델을 활용한 비지도 사전학습을 통해 학습한 많은 가중치들을 활용해 다양한 자연어 처리 모델을 파인튜닝하는 방법이 각광받고 있다. 


## 버트(BERT)
[BERT: Pre-Traing of Deep Bidirectional Transformers for Language Understanding]논문에서 제안된 모델이다. 
> __비지도 사전 학습을 한 모델에 추가로 하나의 완전 연결 계층만 추가한 후__ 
> 파인튜닝을 통해 최고 성능을 보여줬다.
> 버트의 가장 큰 특징은 "양방향성"이다

 BERT, GPT, ELMo를 비교하면,
 GPT는 단방향성, ELMo는 단방향성 2개의 모델을 합친 불완전 양방향성을 가진다.
<img src="https://cdn.aitimes.kr/news/photo/201901/13117_13464_48.jpg">

> 버트는 사전 학습하는 모델이 양방향성(bidirectional)으로 하나의 모델로도 양방향성을 학습한다
{: .prompt-warning }
버트의 사전 학습 문제 중 하나인 마스크 언어 모델(Masked language modeling)을 학습하기 때문에 양방향성을 가지고 학습한다.


### 버트의 사전 학습 문제
언어 모델이란 __단어들의 시퀀스에 대한 확률 분포__ 다.
즉, 단어들의 모음이 있을 때, 해당 단어의 모음이 __어떤 확률로 등장할 지를 나타내는 값__ 이다.

#### 고전적 언어 모델: CBOW 모델
`word2vec` CBOW 모델을 생각해보면, 특정 위치 `주변에 있는 단어가 주어졌을 때`, `특정 위치에 나올 단어를 예측하는 모델`이다. 이는 주변 단어들을 포함한 __단어들의 수열의 확률을 예측__ 하는 모델인 것이다.

- 목적함수: 주변 단어들을 통한 중앙 단어 예측이 학습의 목적.
  > logP(Wt ㅣ Wt-c, Wt-c+1, ...., Wt+c-1, Wt+c)

  t번째 단어를 예측하기 위해 해당 단어의 앞 c개의 단어와 뒤 c개의 단어를 사용한다. 앞뒤로 __`총 2c개의 단어 모음이 있을 때, t번째 위치에 올 단어에 대한 확률 분포를 찾는 언어 모델`__ 이다.
  
  >이처럼, 단어들의 모음에 대한 확률 분포를 찾는 것을 언어 모델 이라고 한다.
  {: .prompt-warning }
  따라서 고전적 언어 모델은 t번째 올 단어들의 확률 분포를 앞선 t-1개의 단어들을 통해 찾는다.

  >'고전적 모델': 앞서 나온 단어들을 통해 바로 직후의 단어가 어떤 단어가 될 지를 예측하는 모델 



#### 버트는 2개의 문제를 사전 학습한다. (마스크, 다음 문장 예측)
__1. 마스크 언어 모델 (양방향성)__
: 입력 문장이 주어진 경우 일부 단어들을 마스킹해서 해당 단어를 모델이 알지 못하도록 가리고, 마스킹 된 단어를 모델이 예측하는 것.
    * 입력값으로 들어간 문장 안의 다른 단어들을 통해 마스킹된 단어를 예측하도록 학습하는 것.
  >버트의 경우 특정 단어를 가리고 앞뒤 상관없이 문장 안의 단어들을 모두 사용해서 가려진 단어를 예측함으로써 양방향의 단어들을 모두 사용함. >(기존: 앞의 단어로 뒤의 단어를 예측함) 


##### 특징

  1. 입력값의 15%의 단어들을 마스킹 
  2. 마스킹 되는 단어의 80%는 [MASK]라는 스페셜 토큰으로 가리고, 10%는 임의의 다른 단어로 대채, 10%는 마스킹하지 않고 단어 그래도 모델로 넣는다.



|입력값         |                                       예시      |
|:------------|:----------------------------------------------:|
|입력값        |딥러닝 자연어 처리 공부는 매우 재밌고 유익하다              |
|마스킹 후보    |딥러닝 자연어 __처리__ 공부는 __매우__ 재밌고 __유익하다__  |
|마스킹 후 입력값|딥러닝 자연어 [MASK] 공부는 __내일__ 재밌고 __유익하다__   |

<br>


__2. 다음 문장 예측__

입력으로 주어진 두 문장이 이어진 문장인지 아닌지를 예측하는 것을 학습.
>버트의 하위 문제 중 두 문장 간의 관계를 예측하는 문제에 도움을 주기 위해 사전 학습함. 
{: .prompt-tip }
➡️ 문장 간의 관계를 모델이 학습하게 됨.


* 데이터셋의 구성 방식<br>
  - 한 문장에 대해 50% 확률로 주어진 문장을 이어서 전체 텍스트를 모델의 입력값으로 넣음
  - 나머지 50% 확률로 한 문장에 임의의 다른 문서의 문장을 이어서 모델의 입력값으로 넣음
  - 주어진 다음 문장인지 여부에 대한 이진 분류 예측을 위해 `[CLS]라는 스페셜 토큰`을 모든 입력값 앞에 함께 넣음
  - 2개의 문장을 함께 입력값으로 하기 때문에, 모델이 각 문장을 구분하도록 `[SEP]라는 스페셜 토큰`을 각 문장이 끝나는 지점에 넣음

버트는 __`마스크 언어 모델, 다음 문장 예측`__ 두 가지를 동시에 사전 학습한다.

마스킹된 단어들을 예측하고, 두 문장이 이어진 문장인지 판단해 두 문장이 이어지지 않았다는 것을 예측한다.

|||
|:------------|:----------------------------------------------:|
|최종적인 버트 모델의 입력값| [CLS]딥러닝 자연어 [MASK] 공부는 [MASK] 재밌고 유익하다[SEP] 내일 날씨는 [MASK] 예정입니다[SEP]                                     |

## 버트 모델
버트모델은 트랜스포머 모델 구조의 __인코더 부분만__ 활용해 모델을 학습한다.
기존 트랜스포머의 포지션-와이즈 피드포워드 네트워크에서 사용됐던 ReLU함수 대신 GELU함수를 사용한다.

### 버트 모델의 하이퍼파라미터
`L` : 트랜스포머 층(블록)의 개수를 의미 (어텐션, 피드 포워드 층을 포함하는 블록이 총 몇번 반복됐는지)

`H` : 모델의 전체적인 차원수를 의미 (전체 층의 출력밧의 차원 수가 동일해야 함)

`A` : 멀티 헤드 어텐션에서의 헤드의 개수를 의미

버트는 사전 학습했던 가중치를 그대로 사용하기 때문에 미세 조정 후의 모델도 동일한 모델 크기를 사용해야 한다. 따라서 큰 모델 사이즈로 사전 학습을 했다면 파인튜닝 또한 학습에 많은 리소소를 필요로 한다.

### 버트 모델 파인튜닝
사전 학습된 가중치를 가지고 있는 버트는 여러 하위 문제에 fine-tuning될 수 있다. 
<img src="https://blog.kakaocdn.net/dn/cEoPYe/btqBW0v9pJo/xM7PQl9BL0XAKX9fYuphw1/img.png">

>사전학습한 버트 모델을 다양한 자연어 처리 하위 문제에 파인튜닝 함으로써 사전 학습 없이 학습한 모델보다 더욱 높은 성능을 얻을 수 있음.

##### 예시
     언어적 용인 가능성, 자연어 추론, 유사도 예측, 감정 분석, 개체명 인식, 기계독해









